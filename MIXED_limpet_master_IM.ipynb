{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for the Mixed-groups image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "###### NB. This notebook can be run in one go or can be exported to a .py file and will also run ######\n",
    "##### NB. This will copy the images and save them to a new dir where it then randomly samples and arranges them into train, val, test folders #####\n",
    "#### NB. Produces model performance metrics in a .csv, Gradcam images and saliency map images ####\n",
    "### NB. the VGG16 additonal layers were tuned using 'keras-tuner' ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Data augmentation\n",
    "import Augmentor\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, backend, optimizers, regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, \n",
    "    LearningRateScheduler, \n",
    "    TensorBoard, \n",
    "    EarlyStopping\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, \n",
    "    MaxPooling2D, \n",
    "    ZeroPadding2D, \n",
    "    Activation, \n",
    "    Flatten, \n",
    "    Dense, \n",
    "    Dropout, \n",
    "    GlobalAveragePooling2D, \n",
    "    BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "# Keras Tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Visualization tools\n",
    "from tf_keras_vis.gradcam import Gradcam\n",
    "from tf_keras_vis.saliency import Saliency\n",
    "from tf_keras_vis.utils.model_modifiers import ReplaceToLinear\n",
    "\n",
    "# Initialize timestamp\n",
    "date = datetime.now().strftime('%Y_%m_%d-%I:%M_%S_%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or set it to '3' to suppress all messages, including INFO and WARNING\n",
    "\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seed for reproducibility\n",
    "SEED = 666\n",
    "\n",
    "# Function to initialize seeds for all libraries which might have stochastic behavior\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Function to ensure partial determinism\n",
    "def set_partial_determinism(seed=SEED):\n",
    "    set_seeds(seed=seed)\n",
    "    \n",
    "    # Comment out the line below if you are facing issues with deterministic operations\n",
    "    # os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    \n",
    "    # Use the following lines if you want to use CPU for deterministic operations\n",
    "    # tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_partial_determinism(seed=SEED)\n",
    "\n",
    "# Ensure XLA is disabled\n",
    "tf.config.optimizer.set_jit(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date = datetime.now().strftime('%Y_%m_%d-%I:%M_%S_%p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the path to the root directory where the subdirectories are located\n",
    "root_dir = 'LS/ventral/' ##etc\n",
    "\n",
    "\n",
    "# Define the path to the new directory to create\n",
    "new_dir = \"../save_offs/LS_ventral_MIXED/pt1\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NB. I set this up to copy only subdirs with at least 150 images to new dir. Good for pruning small classes #####\n",
    "\n",
    "\n",
    "# Create the new directory if it does not exist\n",
    "if not os.path.exists(new_dir):\n",
    "    os.makedirs(new_dir)\n",
    "\n",
    "# Iterate through the subdirectories in the root directory\n",
    "for subdir in os.listdir(root_dir):\n",
    "    # Construct the full path to the subdirectory\n",
    "    subdir_path = os.path.join(root_dir, subdir)\n",
    "    \n",
    "    # Check if the subdirectory contains at least 200 images\n",
    "    if len(os.listdir(subdir_path)) >= 1:\n",
    "        # Copy the subdirectory and its contents to the new directory\n",
    "        shutil.copytree(subdir_path, os.path.join(new_dir, subdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NB. The following code prunes images from specific species-location combinations to ensure no more than 100 images per combination #####\n",
    "\n",
    "\n",
    "pruned_dir = \"../save_offs/LS_ventral_MIXED/pt2/pruned_directory/\"\n",
    "if not os.path.exists(pruned_dir):\n",
    "    os.makedirs(pruned_dir)\n",
    "\n",
    "# Species and their location codes for pruning\n",
    "prune_specs = {\n",
    "    'fv': ['pa'],\n",
    "    'lc': ['ba', 'pa', 'sj'],\n",
    "    'ls': ['pdc', 'pm']\n",
    "}\n",
    "\n",
    "# Iterate through the subdirectories in the new directory\n",
    "for subdir in os.listdir(new_dir):\n",
    "    subdir_path = os.path.join(new_dir, subdir)\n",
    "    \n",
    "    # Ensure the path is a directory\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Iterate through the files in each subdirectory\n",
    "        species_images = {}\n",
    "        \n",
    "        for filename in os.listdir(subdir_path):\n",
    "            # Check if the file is an image\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                # Extract species code and location code from filename\n",
    "                parts = filename.split('_')\n",
    "                if len(parts) >= 3:\n",
    "                    species_code = parts[0]\n",
    "                    location_code = parts[1]\n",
    "\n",
    "                    # Check if the species needs pruning at this location\n",
    "                    if species_code in prune_specs and location_code in prune_specs[species_code]:\n",
    "                        # Add the image path to the dictionary for this species-location combination\n",
    "                        key = f\"{species_code}_{location_code}\"\n",
    "                        if key not in species_images:\n",
    "                            species_images[key] = []\n",
    "                        species_images[key].append(os.path.join(subdir_path, filename))\n",
    "        \n",
    "        # Prune images to move any excess images beyond 100 per species-location\n",
    "        for key, images in species_images.items():\n",
    "            if len(images) > 100:\n",
    "                # Randomly select images to move to pruned directory\n",
    "                images_to_move = random.sample(images, len(images) - 100)\n",
    "\n",
    "                # Create a directory in pruned_dir to save the pruned images\n",
    "                species_code, location_code = key.split('_')\n",
    "                save_dir = os.path.join(pruned_dir, species_code, location_code)\n",
    "                if not os.path.exists(save_dir):\n",
    "                    os.makedirs(save_dir)\n",
    "\n",
    "                # Move the excess images to the pruned directory\n",
    "                for image in images_to_move:\n",
    "                    shutil.move(image, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define source directory and destination directory paths\n",
    "src_dir = \"../save_offs/LS_ventral_MIXED/pt1\"  # Update to the correct source directory path\n",
    "dst_dir = \"../save_offs/LS_ventral_MIXED/pt2\"\n",
    "\n",
    "# Check if the source directory exists\n",
    "if not os.path.exists(src_dir):\n",
    "    raise FileNotFoundError(f\"Source directory '{src_dir}' does not exist. Please check the path and try again.\")\n",
    "\n",
    "# Configurable number of training and validation samples\n",
    "num_train_samples_per_class = 120  # Changeable parameter for training samples\n",
    "num_val_samples_per_class = 30     # Changeable parameter for validation samples\n",
    "\n",
    "# Create train, validation, and test directories if they do not exist\n",
    "train_dir = os.path.join(dst_dir, \"train\")\n",
    "val_dir = os.path.join(dst_dir, \"validation\")\n",
    "test_dir = os.path.join(dst_dir, \"test\")\n",
    "for d in [train_dir, val_dir, test_dir]:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d)\n",
    "\n",
    "# Get the list of classes (subdirectories) in the source directory\n",
    "classes = [subdir for subdir in os.listdir(src_dir) if os.path.isdir(os.path.join(src_dir, subdir))]\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Create new mixed class directories in train and validation folders (test remains original)\n",
    "mixed_class_names = [f\"mixed_class_{i+1}\" for i in range(num_classes)]\n",
    "for mixed_class in mixed_class_names:\n",
    "    for base_dir in [train_dir, val_dir]:\n",
    "        mixed_class_dir = os.path.join(base_dir, mixed_class)\n",
    "        if not os.path.exists(mixed_class_dir):\n",
    "            os.makedirs(mixed_class_dir)\n",
    "\n",
    "# Loop through each class and gather equal numbers of images\n",
    "class_images = {}\n",
    "for class_name in classes:\n",
    "    class_path = os.path.join(src_dir, class_name)\n",
    "    image_files = [f for f in os.listdir(class_path) if f.endswith(\".jpg\")]\n",
    "    class_images[class_name] = image_files\n",
    "\n",
    "# Determine the minimum number of images available across all classes\n",
    "min_images_per_class = min(len(images) for images in class_images.values())\n",
    "\n",
    "# Calculate the number of images to use per set (train/val/test) from each class\n",
    "num_train = min(num_train_samples_per_class, min_images_per_class)\n",
    "num_val = min(num_val_samples_per_class, min_images_per_class - num_train)\n",
    "num_test = min(min_images_per_class - num_train - num_val, min_images_per_class)\n",
    "\n",
    "# Sample images from each class and distribute them into mixed classes for training and validation, and original classes for test\n",
    "for class_name, images in class_images.items():\n",
    "    random.shuffle(images)\n",
    "    train_samples = images[:num_train]\n",
    "    val_samples = images[num_train:num_train + num_val]\n",
    "    test_samples = images[num_train + num_val:num_train + num_val + num_test]\n",
    "\n",
    "    # Distribute images into mixed classes for training and validation\n",
    "    for i, mixed_class in enumerate(mixed_class_names):\n",
    "        # Assign training images\n",
    "        train_dst_dir = os.path.join(train_dir, mixed_class)\n",
    "        for img in train_samples[i::num_classes]:  # Distribute evenly across mixed classes\n",
    "            src_path = os.path.join(src_dir, class_name, img)\n",
    "            dst_path = os.path.join(train_dst_dir, img)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "        # Assign validation images\n",
    "        val_dst_dir = os.path.join(val_dir, mixed_class)\n",
    "        for img in val_samples[i::num_classes]:\n",
    "            src_path = os.path.join(src_dir, class_name, img)\n",
    "            dst_path = os.path.join(val_dst_dir, img)\n",
    "            shutil.copy(src_path, dst_path)\n",
    "\n",
    "    # Assign test images to original test class directories\n",
    "    test_dst_dir = os.path.join(test_dir, class_name)\n",
    "    if not os.path.exists(test_dst_dir):\n",
    "        os.makedirs(test_dst_dir)\n",
    "    for img in test_samples:\n",
    "        src_path = os.path.join(src_dir, class_name, img)\n",
    "        dst_path = os.path.join(test_dst_dir, img)\n",
    "        shutil.copy(src_path, dst_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a new directory called \"test_even\" within the destination directory\n",
    "test_dir = os.path.join(dst_dir, \"test\")\n",
    "test_even_dir = os.path.join(dst_dir, \"test_even\")\n",
    "if not os.path.exists(test_even_dir):\n",
    "    os.makedirs(test_even_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths\n",
    "pruned_dir = \"../save_offs/LS_ventral_MIXED/pt2/pruned_directory/\"\n",
    "test_dir = \"../save_offs/LS_ventral_MIXED/pt2/test/\"\n",
    "\n",
    "# Ensure the pruned directory exists\n",
    "if not os.path.exists(pruned_dir):\n",
    "    raise FileNotFoundError(f\"Pruned directory does not exist: {pruned_dir}\")\n",
    "\n",
    "# Get the 'south_' subdirectory\n",
    "south_subdir = next((d for d in os.listdir(test_dir) if 'south' in d.lower() and os.path.isdir(os.path.join(test_dir, d))), None)\n",
    "\n",
    "if not south_subdir:\n",
    "    raise FileNotFoundError(\"No target subdirectory found in the test directory with 'south' in the name.\")\n",
    "\n",
    "# Debug: print the identified 'south_' subdirectory\n",
    "print(f\"Identified target subdirectory for moving images: {south_subdir}\")\n",
    "\n",
    "# Organize pruned images back into the 'south_' test set folder\n",
    "for root, dirs, files in os.walk(pruned_dir):\n",
    "    for filename in files:\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "            src_path = os.path.join(root, filename)\n",
    "            dst_path = os.path.join(test_dir, south_subdir, filename)\n",
    "\n",
    "            try:\n",
    "                # Move the file to the destination directory\n",
    "                print(f\"Attempting to move {src_path} to {dst_path}\")\n",
    "                shutil.move(src_path, dst_path)\n",
    "                print(f\"Moved {src_path} to {dst_path}\")\n",
    "            except PermissionError as pe:\n",
    "                print(f\"Permission error while moving {src_path} to {dst_path}: {pe}\")\n",
    "            except FileNotFoundError as fnfe:\n",
    "                print(f\"File not found error while moving {src_path} to {dst_path}: {fnfe}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to move {src_path} to {dst_path}: {e}\")\n",
    "\n",
    "# Verify that no files remain in the pruned directory\n",
    "def verify_pruned_directory_empty(pruned_dir):\n",
    "    for root, dirs, files in os.walk(pruned_dir):\n",
    "        if files:\n",
    "            print(f\"Files still remaining in pruned directory: {root}\")\n",
    "            for file in files:\n",
    "                print(f\" - {file}\")\n",
    "        else:\n",
    "            print(f\"No files remaining in pruned directory: {root}\")\n",
    "\n",
    "verify_pruned_directory_empty(pruned_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through subdirectories in test directory\n",
    "for subdir in os.listdir(test_dir):\n",
    "    subdir_path = os.path.join(test_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Create identical subdirectories within test_even directory\n",
    "        test_even_subdir = os.path.join(test_even_dir, subdir)\n",
    "        if not os.path.exists(test_even_subdir):\n",
    "            os.makedirs(test_even_subdir)\n",
    "\n",
    "        # Randomly sample 20 images from the subdirectory\n",
    "        image_files = [f for f in os.listdir(subdir_path) if f.endswith(\".jpg\")]\n",
    "        num_images = len(image_files)\n",
    "        if num_images < 20:\n",
    "            num_samples = num_images\n",
    "        else:\n",
    "            num_samples = 20\n",
    "        sample_files = random.sample(image_files, num_samples)\n",
    "\n",
    "        # Copy sampled images to the test_even subdirectory\n",
    "        for sample_file in sample_files:\n",
    "            src_path = os.path.join(subdir_path, sample_file)\n",
    "            dst_path = os.path.join(test_even_subdir, sample_file)\n",
    "            shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dir = '../save_offs/LS_ventral_MIXED/pt2/train/'\n",
    "\n",
    "# Get a list of all subdirectories within the main directory\n",
    "subdirs = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "\n",
    "# Loop over each subdirectory and apply augmentations\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(dir, subdir)\n",
    "    p = Augmentor.Pipeline(subdir_path, output_directory='')\n",
    "    p.flip_left_right(probability=0.5)\n",
    "    p.flip_top_bottom(probability=0.5)\n",
    "    #p.rotate(probability=0.9,max_left_rotation=18,max_right_rotation=18)\n",
    "    p.rotate90(probability=0.5)\n",
    "    p.rotate180(probability=0.5)\n",
    "    p.rotate270(probability=0.5)\n",
    "    #p.scale(probability=0.5, scale_factor=1.5)\n",
    "    p.random_brightness(probability=0.5,min_factor=0.5,max_factor=1.5)\n",
    "    p.random_contrast(probability=0.5,min_factor=0.5,max_factor=1.5)\n",
    "\n",
    "    # Get the number of files in the directory\n",
    "    num_files = len(os.listdir(subdir_path))\n",
    "\n",
    "    # Sample additional images if necessary\n",
    "    num_samples = 2400 - num_files\n",
    "    if num_samples > 0:\n",
    "        p.sample(num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '../save_offs/LS_ventral_MIXED/pt2/validation/'\n",
    "\n",
    "# Get a list of all subdirectories within the main directory\n",
    "subdirs = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "\n",
    "# Loop over each subdirectory and apply augmentations\n",
    "for subdir in subdirs:\n",
    "    subdir_path = os.path.join(dir, subdir)\n",
    "    p = Augmentor.Pipeline(subdir_path, output_directory='')\n",
    "    p.flip_left_right(probability=0.5)\n",
    "    p.flip_top_bottom(probability=0.5)\n",
    "    #p.rotate(probability=0.9,max_left_rotation=18,max_right_rotation=18)\n",
    "    p.rotate90(probability=0.5)\n",
    "    p.rotate180(probability=0.5)\n",
    "    p.rotate270(probability=0.5)\n",
    "    #p.scale(probability=0.5, scale_factor=1.5)\n",
    "    p.random_brightness(probability=0.5,min_factor=0.5,max_factor=1.5)\n",
    "    p.random_contrast(probability=0.5,min_factor=0.5,max_factor=1.5)\n",
    "\n",
    "    # Get the number of files in the directory\n",
    "    num_files = len(os.listdir(subdir_path))\n",
    "\n",
    "    # Sample additional images if necessary\n",
    "    num_samples = 600 - num_files\n",
    "    if num_samples > 0:\n",
    "        p.sample(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your data directories\n",
    "train_dir = '../save_offs/LS_ventral_MIXED/pt2/train/'\n",
    "validation_dir = '../save_offs/LS_ventral_MIXED/pt2/validation/'\n",
    "test_dir = '../save_offs/LS_ventral_MIXED/pt2/test_even/'\n",
    "test_full_dir = '../save_offs/LS_ventral_MIXED/pt2/test/'\n",
    "\n",
    "\n",
    "img_width, img_height = 224, 224\n",
    "BATCHSIZE = 16\n",
    "\n",
    "# Define a function to preprocess the images\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, (img_width, img_height))\n",
    "    image = image / 255.0\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train data\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image\n",
    ")\n",
    "\n",
    "val_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_image\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=BATCHSIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "# Load the validation data\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=BATCHSIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Load the test data\n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=BATCHSIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "test_full_datagen = keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocess_image)\n",
    "test_full_generator = test_full_datagen.flow_from_directory(\n",
    "    test_full_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=BATCHSIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "# Store the class names in a list\n",
    "class_names = list(train_generator.class_indices.keys())\n",
    "n_classes = len(class_names)\n",
    "print(f'Class names: {class_names}')\n",
    "print('Num of classes:', n_classes)\n",
    "\n",
    "# Store the number of images in each set\n",
    "train_set_size = train_generator.n\n",
    "validation_set_size = validation_generator.n\n",
    "test_set_size = test_generator.n\n",
    "test_full_set_size = test_full_generator.n\n",
    "\n",
    "print(\"Train set size:\", train_set_size)\n",
    "print(\"Validation set size:\", validation_set_size)\n",
    "print(\"Test set size:\", test_set_size)\n",
    "print('test_full_size:', test_full_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.VGG16(input_shape=(224, 224, 3),\n",
    "                                   weights = 'imagenet',\n",
    "                                   include_top = False\n",
    "                                   )\n",
    "\n",
    "\n",
    "X= model.layers[-1].output\n",
    "\n",
    "# Additonal layers can be added and changed from here\n",
    "X = tf.keras.layers.GlobalAveragePooling2D()(X)\n",
    "X = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(X)\n",
    "X = tf.keras.layers.Dropout(0.25)(X)\n",
    "X = tf.keras.layers.Dense(288, activation='relu'\n",
    "                         )(X)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dropout(0.2)(X)\n",
    "\n",
    "predictions = Dense(n_classes, activation=\"softmax\"\n",
    "                   )(X)\n",
    "\n",
    "\n",
    "model_final = Model(model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layers in (model.layers)[:-1]:\n",
    "    print(layers)\n",
    "    layers.trainable = False\n",
    "    \n",
    "for layer in model_final.layers:\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(model_final.layers):\n",
    "    print(\"Layer: {}, Trainable: {}\".format(index, layer.trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '../save_offs/LS_ventral_MIXED/limpet_aug_mk1'+date+'.keras'\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "#checkpoint = ModelCheckpoint(monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq = 'epochs')\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path,save_best_only = True, monitor='val_accuracy' ,save_weights_only = False, verbose = 1)\n",
    "early = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=2, verbose=4, mode='max')\n",
    "\n",
    "base_learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_final.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy',\n",
    "                       #keras.metrics.SparseTopKCategoricalAccuracy(k=3)\n",
    "                      ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_exp_decay(initial_epochs, lr):\n",
    "    k = 0.01\n",
    "    return base_learning_rate * math.exp(-k*initial_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_final.fit(train_generator,\n",
    "                          epochs= initial_epochs,\n",
    "                          validation_data= validation_generator,\n",
    "                          callbacks=[cp_callback,\n",
    "                                     early,\n",
    "                                    ],\n",
    "                         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_final.evaluate(\n",
    "    test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_final.layers:\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "\n",
    "for index, layer in enumerate(model_final.layers):\n",
    "    print(\"Layer: {}, Trainable: {}\".format(index, layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(model_final.layers))\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 8\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in model_final.layers[:fine_tune_at]:\n",
    "  layer.trainable = False\n",
    "\n",
    "for layer in model_final.layers:\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(model_final.layers):\n",
    "    print(\"Layer: {}, Trainable: {}\".format(index, layer.trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate has been lowered as more model ahs been opened for training. Should stop overfititng\n",
    "\n",
    "TL_learningRate = base_learning_rate/10\n",
    "\n",
    "model_final.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=TL_learningRate),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_epochs = 200\n",
    "\n",
    "total_epochs = initial_epochs + fine_tune_epochs\n",
    "\n",
    "#np.random.seed(343)\n",
    "# fit the model\n",
    "history_fine = model_final.fit(\n",
    "  train_generator,\n",
    "  epochs=total_epochs,\n",
    "  initial_epoch = history.epoch[-1],\n",
    "  #initial_epoch = fine_tune_epochs,\n",
    "  validation_data=validation_generator,\n",
    "  callbacks=[cp_callback, early,\n",
    "             #LearningRateScheduler(TF_lr_exp_decay, verbose=1)\n",
    "            ],\n",
    "  #batch_size=128,\n",
    "  #shuffle=True\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model_final.evaluate(\n",
    "    test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_str = str(round(test_acc * 100,2))\n",
    "\n",
    "\n",
    "model_name = f'../save_offs/LS_ventral_MIXED/fvDorsal_nVSs{acc_str}_{date}.keras'\n",
    "\n",
    "model_final.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_final.predict(test_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_names[idx] for idx in predicted_class_indices]\n",
    "\n",
    "# Get true class labels\n",
    "true_classes = test_generator.classes\n",
    "class_labels = list(test_generator.class_indices.keys())\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "confusion_mtx = confusion_matrix(true_classes, predicted_class_indices)\n",
    "class_report = classification_report(true_classes, predicted_class_indices, target_names=class_labels, output_dict=True)\n",
    "\n",
    "# Convert confusion matrix and classification report to DataFrame\n",
    "confusion_mtx_df = pd.DataFrame(confusion_mtx, index=class_labels, columns=class_labels)\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "# Save confusion matrix and classification report to Excel\n",
    "with pd.ExcelWriter('../save_offs/LS_ventral_MIXED/classification_results_test.xlsx') as writer:  \n",
    "    confusion_mtx_df.to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    class_report_df.to_excel(writer, sheet_name='Classification Report')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model_final.predict(test_full_generator)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_class_indices = np.argmax(predictions, axis=1)\n",
    "predicted_classes = [class_names[idx] for idx in predicted_class_indices]\n",
    "\n",
    "# Get true class labels\n",
    "true_classes = test_full_generator.classes\n",
    "class_labels = list(test_full_generator.class_indices.keys())\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "confusion_mtx = confusion_matrix(true_classes, predicted_class_indices)\n",
    "class_report = classification_report(true_classes, predicted_class_indices, target_names=class_labels, output_dict=True)\n",
    "\n",
    "# Convert confusion matrix and classification report to DataFrame\n",
    "confusion_mtx_df = pd.DataFrame(confusion_mtx, index=class_labels, columns=class_labels)\n",
    "class_report_df = pd.DataFrame(class_report).transpose()\n",
    "\n",
    "# Save confusion matrix and classification report to Excel\n",
    "with pd.ExcelWriter('../save_offs/LS_ventral_MIXED/classification_results_test_full.xlsx') as writer:  \n",
    "    confusion_mtx_df.to_excel(writer, sheet_name='Confusion Matrix')\n",
    "    class_report_df.to_excel(writer, sheet_name='Classification Report')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the path to the folder containing the images\n",
    "img_folder_path = '../save_offs/LS_ventral_MIXED/pt2/test_even/'\n",
    "\n",
    "# Set the path to the folder where the predicted images will be saved\n",
    "output_folder_path = '../save_offs/LS_ventral_MIXED/predicted_images'\n",
    "\n",
    "# Create an ImageDataGenerator instance\n",
    "datagen = ImageDataGenerator()\n",
    "generator = datagen.flow_from_directory(img_folder_path, target_size=(img_width, img_height), batch_size=1, shuffle=False, class_mode='categorical')\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results_list = []\n",
    "\n",
    "# Iterate through the subdirectories in the folder\n",
    "for sub_dir_name in os.listdir(img_folder_path):\n",
    "    sub_dir_path = os.path.join(img_folder_path, sub_dir_name)\n",
    "    if os.path.isdir(sub_dir_path):\n",
    "        # Create a corresponding subdirectory in the output folder\n",
    "        output_sub_dir_path = os.path.join(output_folder_path, sub_dir_name)\n",
    "        if not os.path.exists(output_sub_dir_path):\n",
    "            os.makedirs(output_sub_dir_path)\n",
    "\n",
    "        # Iterate through the images in the subdirectory\n",
    "        for img_file in os.listdir(sub_dir_path):\n",
    "            if img_file.endswith('.jpg'):  # Consider only JPG files\n",
    "                # Read the image, convert to RGB color space, and resize\n",
    "                img_path = os.path.join(sub_dir_path, img_file)\n",
    "                with Image.open(img_path) as img:\n",
    "                    img = img.convert('RGB')\n",
    "                    img = img.resize((img_width, img_height))\n",
    "\n",
    "                # Make a prediction on the image\n",
    "                img_array = np.asarray(img)\n",
    "                img_array = preprocess_image(img_array)\n",
    "                prediction = model_final.predict(np.expand_dims(img_array, axis=0))[0]\n",
    "                predicted_class_idx = np.argmax(prediction)\n",
    "                predicted_class = class_names[predicted_class_idx]\n",
    "                confidence_score = prediction[predicted_class_idx]\n",
    "\n",
    "                # Save a copy of the image to the corresponding predicted class subdirectory\n",
    "                output_sub_dir_class_path = os.path.join(output_sub_dir_path, str(predicted_class_idx))\n",
    "                if not os.path.exists(output_sub_dir_class_path):\n",
    "                    os.makedirs(output_sub_dir_class_path)\n",
    "                output_img_path = os.path.join(output_sub_dir_class_path, img_file)\n",
    "                img.save(output_img_path)\n",
    "\n",
    "                # Add the result to the list\n",
    "                actual_class = generator.class_indices[sub_dir_name]\n",
    "                result_dict = {\n",
    "                    'Image': img_file,\n",
    "                    'Actual_Class': actual_class,\n",
    "                    'Predicted_Class': predicted_class,\n",
    "                    'Confidence_Score': confidence_score\n",
    "                }\n",
    "                results_list.append(result_dict)\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Save the results to a CSV file named 'predicted_results.csv'\n",
    "results_df.to_csv('../save_offs/LS_ventral_MIXED/predicted_images/predicted_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace2linear = ReplaceToLinear()\n",
    "\n",
    "gradcam = Gradcam(model_final,\n",
    "                  model_modifier=replace2linear,\n",
    "                  clone=True)\n",
    "\n",
    "def score_function(output):\n",
    "    return output[0][class_index]\n",
    "\n",
    "img_folder_path = '../save_offs/LS_ventral_MIXED/predicted_images'\n",
    "\n",
    "# Create heatmap directory\n",
    "heatmap_dir = '../save_offs/LS_ventral_MIXED/true_heatmaps/'\n",
    "os.makedirs(heatmap_dir, exist_ok=True)\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Image_Name', 'Subdirectory', 'Class_Index', 'Heatmap_Intensity'])\n",
    "\n",
    "# Existing batch_size declaration\n",
    "BATCHSIZE = 32  # You can adjust this value based on your available memory\n",
    "\n",
    "for sub_dir_name in os.listdir(img_folder_path):\n",
    "    sub_dir_path = os.path.join(img_folder_path, sub_dir_name)\n",
    "    if os.path.isdir(sub_dir_path):\n",
    "        for sub_sub_dir_name in os.listdir(sub_dir_path):\n",
    "            sub_sub_dir_path = os.path.join(sub_dir_path, sub_sub_dir_name)\n",
    "            if os.path.isdir(sub_sub_dir_path):\n",
    "                class_index = int(sub_sub_dir_name)\n",
    "                print(f\"Processing images in sub-subdirectory {sub_sub_dir_path}, using class index {class_index}\")\n",
    "\n",
    "                # Get the list of image files in the sub-sub-directory\n",
    "                img_files = [img_file for img_file in os.listdir(sub_sub_dir_path) if img_file.endswith('.jpg')]\n",
    "\n",
    "                # Process images in batches\n",
    "                for i in range(0, len(img_files), BATCHSIZE):\n",
    "                    batch_img_files = img_files[i:i+BATCHSIZE]\n",
    "                    for img_file in batch_img_files:\n",
    "                        img_path = os.path.join(sub_sub_dir_path, img_file)\n",
    "                        img = load_img(img_path, target_size=(224, 224))\n",
    "                        x = np.array(img)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "\n",
    "                        cam = gradcam(score_function, x, penultimate_layer=-1)\n",
    "\n",
    "                        heatmap_quantified = np.sum(cam[0])\n",
    "\n",
    "                        # Add results to dataframe\n",
    "                        df = pd.concat([df, pd.DataFrame([{'Image_Name': img_file, 'Subdirectory': sub_dir_name, 'Class_Index': class_index, 'Heatmap_Intensity': heatmap_quantified}])], ignore_index=True)\n",
    "\n",
    "                        heatmap = np.uint8(cm.jet(cam[0])[..., :3] * 255)\n",
    "                        plt.imshow(img)\n",
    "                        plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "                        plt.axis('off')\n",
    "                        plt.title(f\"{img_file}, Heatmap intensity: {heatmap_quantified}\")\n",
    "\n",
    "                        # Save the image\n",
    "                        heatmap_save_dir = os.path.join(heatmap_dir, sub_dir_name, sub_sub_dir_name)\n",
    "                        os.makedirs(heatmap_save_dir, exist_ok=True)\n",
    "                        save_path = os.path.join(heatmap_save_dir, f\"{img_file}_heatmap.jpg\")\n",
    "                        plt.savefig(save_path)\n",
    "                        plt.close()\n",
    "\n",
    "                    # After processing each batch, clear memory\n",
    "                    K.clear_session()\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df = df[['Image_Name', 'Subdirectory', 'Class_Index', 'Heatmap_Intensity']]\n",
    "\n",
    "# Define the path where you want to save the Excel file\n",
    "excel_file_path = \"../save_offs/LS_ventral_MIXED/heatmap_results.xlsx\"\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df.to_excel(excel_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace2linear = ReplaceToLinear()\n",
    "\n",
    "# Create the Saliency object\n",
    "saliency = Saliency(model_final,\n",
    "                    model_modifier=replace2linear,\n",
    "                    clone=True)\n",
    "\n",
    "def score_function(output):\n",
    "    return output[0][class_index]\n",
    "\n",
    "img_folder_path = '../save_offs/LS_ventral_MIXED/predicted_images'\n",
    "\n",
    "# Create heatmap directory\n",
    "heatmap_dir = '../save_offs/LS_ventral_MIXED/true_saliency_maps/'\n",
    "os.makedirs(heatmap_dir, exist_ok=True)\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df = pd.DataFrame(columns=['Image_Name', 'Subdirectory', 'Class_Index', 'Saliency_Intensity'])\n",
    "\n",
    "# Existing batch_size declaration\n",
    "BATCHSIZE = 32  # You can adjust this value based on your available memory\n",
    "\n",
    "for sub_dir_name in os.listdir(img_folder_path):\n",
    "    sub_dir_path = os.path.join(img_folder_path, sub_dir_name)\n",
    "    if os.path.isdir(sub_dir_path):\n",
    "        for sub_sub_dir_name in os.listdir(sub_dir_path):\n",
    "            sub_sub_dir_path = os.path.join(sub_dir_path, sub_sub_dir_name)\n",
    "            if os.path.isdir(sub_sub_dir_path):\n",
    "                class_index = int(sub_sub_dir_name)\n",
    "                print(f\"Processing images in sub-subdirectory {sub_sub_dir_path}, using class index {class_index}\")\n",
    "\n",
    "                # Get the list of image files in the sub-sub-directory\n",
    "                img_files = [img_file for img_file in os.listdir(sub_sub_dir_path) if img_file.endswith('.jpg')]\n",
    "\n",
    "                # Process images in batches\n",
    "                for i in range(0, len(img_files), BATCHSIZE):\n",
    "                    batch_img_files = img_files[i:i+BATCHSIZE]\n",
    "                    for img_file in batch_img_files:\n",
    "                        img_path = os.path.join(sub_sub_dir_path, img_file)\n",
    "\n",
    "                        # Load the image at original size\n",
    "                        img = Image.open(img_path)\n",
    "                        original_size = img.size\n",
    "\n",
    "                        # Resize image to model input size for saliency\n",
    "                        img_resized = img.resize((224, 224))\n",
    "                        x = img_to_array(img_resized)\n",
    "                        x = np.expand_dims(x, axis=0)\n",
    "                        x = preprocess_input(x)\n",
    "\n",
    "                        # Generate the saliency map\n",
    "                        saliency_map = saliency(score_function,\n",
    "                                                x,\n",
    "                                                smooth_samples=20,  # Number of gradient iterations\n",
    "                                                smooth_noise=0.20)  # Noise spread level\n",
    "\n",
    "                        # Calculate saliency map intensity as a sum\n",
    "                        saliency_intensity = np.sum(saliency_map[0])\n",
    "\n",
    "                        # Add results to dataframe\n",
    "                        df = pd.concat([df, pd.DataFrame([{'Image_Name': img_file, 'Subdirectory': sub_dir_name, 'Class_Index': class_index, 'Saliency_Intensity': saliency_intensity}])], ignore_index=True)\n",
    "\n",
    "                        # Resize the saliency map to the original image size\n",
    "                        saliency_resized = np.uint8(cm.jet(saliency_map[0])[..., :3] * 255)\n",
    "                        saliency_resized = Image.fromarray(saliency_resized).resize(original_size)\n",
    "\n",
    "                        # Overlay saliency map on original image\n",
    "                        plt.imshow(img)\n",
    "                        plt.imshow(saliency_resized, cmap='jet', alpha=0.5)\n",
    "                        plt.axis('off')\n",
    "                        plt.title(f\"{img_file}, Saliency intensity: {saliency_intensity}\")\n",
    "\n",
    "                        # Save the image\n",
    "                        saliency_save_dir = os.path.join(heatmap_dir, sub_dir_name, sub_sub_dir_name)\n",
    "                        os.makedirs(saliency_save_dir, exist_ok=True)\n",
    "                        save_path = os.path.join(saliency_save_dir, f\"{img_file}_saliency.jpg\")\n",
    "                        plt.savefig(save_path)\n",
    "                        plt.close()\n",
    "\n",
    "                    # After processing each batch, clear memory\n",
    "                    K.clear_session()\n",
    "\n",
    "# Reorder the DataFrame columns\n",
    "df = df[['Image_Name', 'Subdirectory', 'Class_Index', 'Saliency_Intensity']]\n",
    "\n",
    "# Define the path where you want to save the Excel file\n",
    "excel_file_path = \"../save_offs/LS_ventral_MIXED/saliency_results.xlsx\"\n",
    "\n",
    "# Save DataFrame to Excel\n",
    "df.to_excel(excel_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
